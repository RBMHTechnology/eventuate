akka {
  actor {
    serializers {
      eventuate-durable-event = "com.rbmhtechnology.eventuate.serializer.DurableEventSerializer"
      eventuate-replication-filter = "com.rbmhtechnology.eventuate.serializer.ReplicationFilterSerializer"
      eventuate-replication-protocol = "com.rbmhtechnology.eventuate.serializer.ReplicationProtocolSerializer"
      eventuate-snapshot = "com.rbmhtechnology.eventuate.serializer.SnapshotSerializer"
    }

    serialization-bindings {
      "com.rbmhtechnology.eventuate.DurableEvent" = eventuate-durable-event
      "com.rbmhtechnology.eventuate.ReplicationFilter$Format" = eventuate-replication-filter
      "com.rbmhtechnology.eventuate.ReplicationProtocol$Format" = eventuate-replication-protocol
      "com.rbmhtechnology.eventuate.ConcurrentVersionsTree" = eventuate-snapshot
      "com.rbmhtechnology.eventuate.Snapshot" = eventuate-snapshot
      "com.rbmhtechnology.eventuate.log.TimeTracker" = eventuate-snapshot
    }
  }
}

eventuate {
  log.batching {
    # This limit is used by the batching layer to limit the number of events
    # to be written atomically to the event log. If the number of events from
    # accumulated write requests exceeds this limit, the batching layer serves
    # the these write requests immediately. Consequently, the size of written
    # event batches may be larger than this limit.
    #
    # Write requests are only accumulated by the batching layer if there is
    # currently another write in progress. If no write is in progress the
    # batching layer serves a write request immediately, regardless whether
    # the batch-size-limit is exceeded or not.
    batch-size-limit = 256
  }

  log.replication {
    # Maximum batch size for event replication and batch writes to the target
    # event log.
    batch-size-max = 512

    # Event replication retry interval. Event replication is re-tried at this
    # interval if previous transfer batch was empty or failed.
    retry-interval = 5s

    # Timeout for reading events from the remote source log.
    read-timeout = 10s

    # Timeout for writing events to the local target log.
    write-timeout = 10s

    # Maximum duration of missing heartbeats from a remote location until
    # that location is considered unavailable.
    failure-detection-limit = 60s
  }

  log.leveldb {
    # Root directory for storing the log directories of individual event logs.
    dir = target

    # Use fsync on write.
    fsync = on

    # Minimum number of new events that must have been written before another
    # snapshot of the log's internal state (sequence number and merged vector
    # time) is written.
    state-snapshot-limit = 128

    write-dispatcher {
      executor = "thread-pool-executor"
      type = PinnedDispatcher
    }

    read-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 8
      }
    }
  }

  log.cassandra {
    # Comma-separated list of contact points in the cluster (comma-separated
    # hostname or hostname:port list).
    contact-points = ["127.0.0.1"]

    # Default port of contact points in the cluster. Ports defined in
    # contact-points override this setting.
    default-port = 9042

    # Default Cassandra username
    username = "cassandra"

    # Default Cassandra password
    password = "cassandra"

    # Name of the keyspace created/used by Eventuate.
    keyspace = "eventuate"

    # Whether or not to auto-create the keyspace.
    keyspace-autocreate = true

    # Prefix of all tables created/used by Eventuate.
    table-prefix = "log"

    # Replication factor to use when creating the keyspace.
    replication-factor = 1

    # Write consistency level
    write-consistency = "QUORUM"

    # Read consistency level
    read-consistency = "QUORUM"

    # Maximum number of events stored per event log partition. Must be greater
    # than eventuate.log.write.batch-size-limit and
    # eventuate.log.replication.batch-size-max
    partition-size-max = 131072

    # Retry backoff for event log initialization. Initialization requires
    # reading the last indexed sequence number and indexing of not yet
    # indexed log entries.
    init-retry-backoff = 5s

    # Minimum number of new events that must have been written before another
    # index update is triggered.
    index-update-limit = 128

    write-dispatcher {
      executor = "thread-pool-executor"
      type = PinnedDispatcher
    }

    read-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 16
      }
    }
  }

  snapshot.filesystem {
    # Root directory for storing the snapshot files of individual emitters.
    dir = target

    # Maximum number of stored snapshots per emitter. If this number is
    # exceeded during a snapshot write, older snapshots are automatically
    # deleted.
    snapshots-per-emitter-max = 3

    write-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 8
      }
    }

    read-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 32
      }
    }
  }

  disaster-recovery {
    # Maximum number of retries of remote operations. These are operations
    # that read from and write to remote replication endpoints.
    remote-operation-retry-max = 3

    # Delay before re-trying a remote operation after a previous failure.
    remote-operation-retry-delay = 10s

    # Timeout of remote operations.
    remote-operation-timeout = 10s

    # Timeout for deleting invalidated snapshots.
    snapshot-deletion-timeout = 30s
  }
}
